{
 "cells": [
  {
   "cell_type": "raw",
   "id": "95ec85b9-e5a6-4beb-82e6-5dc52500dfd0",
   "metadata": {},
   "source": [
    "Tested with:\n",
    "- python 3.11.5\n",
    "- gymnasium 0.28.1\n",
    "- pytorch 2.1.1\n",
    "- numpy 1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e63e6-8541-4ad3-ab92-c5771261d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38add53d-ce16-4f0c-972c-aa95371f003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = lambda inp: inp.cuda() if USE_CUDA else inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794b2e3-f04e-4c66-ae65-da93cd8faf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT\n",
    "\n",
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38843a0a-2d02-4050-9405-369936ddfec8",
   "metadata": {},
   "source": [
    "Prioritized experience replay\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1511.05952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ee606-5be9-44e1-9043-cb333810ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REPLAY BUFFER\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, num_training, prob_alpha=0.6, beta_0=0.4):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.beta_0 = beta_0\n",
    "        self.capacity = capacity\n",
    "        self.num_training = num_training\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.prio_alp = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.max_prio_alp = 1.\n",
    "    \n",
    "    def push(self, batch_state, batch_action, batch_reward, batch_next_state, batch_done):\n",
    "        num_samples = batch_state.shape[0]\n",
    "        buffer_len = len(self.buffer)\n",
    "        pos = (self.pos + np.arange(num_samples)) % self.capacity\n",
    "        \n",
    "        if buffer_len < self.capacity:\n",
    "            for i in range(num_samples):\n",
    "                if (buffer_len + i) < self.capacity:\n",
    "                    self.buffer.append(\n",
    "                        (batch_state[i].reshape(1,-1),\n",
    "                         batch_action[i],\n",
    "                         batch_reward[i],\n",
    "                         batch_next_state[i].reshape(1,-1),\n",
    "                         batch_done[i])\n",
    "                    )\n",
    "                else:\n",
    "                    if self.prio_alp[pos[i]] == self.max_prio_alp:\n",
    "                        mask = np.ones_like(self.prio_alp)\n",
    "                        mask[pos[i]] = 0\n",
    "                        self.max_prio_alp = (self.prio_alp * mask).max()\n",
    "                    self.buffer[pos[i]] = (\n",
    "                        batch_state[i].reshape(1,-1),\n",
    "                        batch_action[i],\n",
    "                        batch_reward[i],\n",
    "                        batch_next_state[i].reshape(1,-1),\n",
    "                        batch_done[i]\n",
    "                    )\n",
    "        else:\n",
    "            if self.prio_alp[pos].max() == self.max_prio_alp:\n",
    "                mask = np.ones_like(self.prio_alp)\n",
    "                mask[pos] = 0\n",
    "                self.max_prio_alp = (self.prio_alp * mask).max()\n",
    "            for i in range(num_samples):\n",
    "                self.buffer[pos[i]] = (\n",
    "                    batch_state[i].reshape(1,-1),\n",
    "                    batch_action[i],\n",
    "                    batch_reward[i],\n",
    "                    batch_next_state[i].reshape(1,-1),\n",
    "                    batch_done[i]\n",
    "                )\n",
    "\n",
    "        self.prio_alp[pos] = self.max_prio_alp\n",
    "        self.pos = (self.pos + num_samples) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, training_idx):\n",
    "        beta = min(1.0, self.beta_0 + training_idx * (1.0 - self.beta_0) / self.num_training)\n",
    "        buffer_len = len(self.buffer)\n",
    "        \n",
    "        if buffer_len == self.capacity:\n",
    "            prio_alps = self.prio_alp\n",
    "        else:\n",
    "            prio_alps = self.prio_alp[:self.pos]\n",
    "\n",
    "        # P_j = p_j^{\\alpha} / {\\sum_i p_i^{\\alpha}}\n",
    "        probs = prio_alps / prio_alps.sum()\n",
    "\n",
    "        indices = np.random.choice(buffer_len, batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # w_j = (N  * P_j)^{-\\beta}\n",
    "        # w_j = w_j / {\\max_i w_i}\n",
    "        weights  = (buffer_len * probs[indices]) ** (-beta)\n",
    "        weights /= (buffer_len * probs.min()) ** (-beta)\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        states      = np.concatenate([val[0] for val in samples])\n",
    "        actions     = [val[1] for val in samples]\n",
    "        rewards     = [val[2] for val in samples]\n",
    "        next_states = np.concatenate([val[3] for val in samples])\n",
    "        dones       = [val[4] for val in samples]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        # p_i^{\\alpha}\n",
    "        prio_alps = batch_priorities ** self.prob_alpha\n",
    "        self.prio_alp[batch_indices] = prio_alps\n",
    "        max_prio_alps = prio_alps.max()\n",
    "        if max_prio_alps > self.max_prio_alp:\n",
    "            self.max_prio_alp = max_prio_alps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6759a-efee-4d21-9857-8c9f20f4604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions)\n",
    "        )\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def eps_act(self, state, epsilon):\n",
    "        select = np.random.rand(state.shape[0]) > epsilon\n",
    "        action = select * self.greedy_act(state) + np.logical_not(select) * np.random.randint(0, self.num_actions, size=state.shape[0])\n",
    "        return action\n",
    "    \n",
    "    def greedy_act(self, state):\n",
    "        state = device(torch.FloatTensor(state))\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "        action = q_values.max(1)[1].cpu().numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2c1bb-91c4-4703-b02a-a262b181005b",
   "metadata": {},
   "source": [
    "DDQN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1509.06461\n",
    "\n",
    "target: $R_{t+1}+{\\gamma}Q\\left(S_{t+1}, \\arg\\max_{a'}{Q\\left(S_{t+1}, a';\\theta_t\\right)};\\theta^-_t\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233bcb-9125-4f2a-b15f-416381cf2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DDQN Agent\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, env_id, eps, gamma, lr, num_frames, rep_buf_size, batch_size, tau):\n",
    "        self.envs = gym.vector.make(env_id, num_envs=batch_size)\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.num_frames = num_frames\n",
    "        self.rep_buf_size = rep_buf_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.model = device(Net(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n))\n",
    "        self.target = device(Net(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n))\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.rep_buf = PrioritizedReplayBuffer(rep_buf_size, num_frames)\n",
    "\n",
    "    def train(self):\n",
    "        losses = []\n",
    "        all_rewards = []\n",
    "        episode_reward = np.zeros(self.batch_size)\n",
    "        \n",
    "        state, _ = self.envs.reset()\n",
    "        for frame_idx in range(0, self.num_frames):\n",
    "            action = self.model.eps_act(state, self.eps)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = self.envs.step(action)\n",
    "            done = np.logical_or(terminated, truncated)\n",
    "            self.rep_buf.push(state, action, reward, next_state, done)\n",
    "\n",
    "            model_w = self.model.state_dict()\n",
    "            target_w = self.target.state_dict()\n",
    "            for key in model_w:\n",
    "                target_w[key] = model_w[key] * self.tau + target_w[key] * (1 - self.tau)\n",
    "            self.target.load_state_dict(target_w)\n",
    "            \n",
    "            loss = self.compute_td_loss(frame_idx)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "            all_rewards.extend(episode_reward[done].tolist())\n",
    "            episode_reward[done] = 0\n",
    "                \n",
    "            if (frame_idx + 1) % 200 == 0:\n",
    "                self.plot_training(frame_idx, all_rewards, losses)\n",
    "        \n",
    "        self.envs.close()\n",
    "\n",
    "    def compute_td_loss(self, frame_idx):\n",
    "        state, action, reward, next_state, done, indices, weights = self.rep_buf.sample(self.batch_size, frame_idx)\n",
    "        \n",
    "        state      = device(torch.FloatTensor(state))\n",
    "        next_state = device(torch.FloatTensor(next_state))\n",
    "        action     = device(torch.LongTensor(action))\n",
    "        reward     = device(torch.FloatTensor(reward))\n",
    "        done       = device(torch.FloatTensor(done))\n",
    "        weights    = device(torch.FloatTensor(weights))\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        q_value  = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_next_q_values = self.model(next_state)\n",
    "        model_next_q_action = model_next_q_values.max(1)[1]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            target_next_q_values = self.target(next_state)\n",
    "        target_next_q_value = target_next_q_values.gather(1, model_next_q_action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        expected_q_value = reward + self.gamma * target_next_q_value * (1 - done)\n",
    "\n",
    "        td_error = expected_q_value - q_value\n",
    "        loss  = td_error.pow(2) * weights\n",
    "        loss  = loss.mean()\n",
    "        \n",
    "        self.rep_buf.update_priorities(indices, np.abs(td_error.detach().cpu().numpy()) + 1e-5)\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(frame_idx, rewards, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('episode: {}, total reward(ma-10): {}'.format(len(rewards), np.mean(rewards[-10:])))\n",
    "        plt.plot(np.array(rewards)[:100 * (len(rewards) // 100)].reshape(-1, 100).mean(axis=1))\n",
    "        plt.subplot(132)\n",
    "        plt.title('frame: {}, loss(ma-10): {:.4f}'.format(frame_idx, np.mean(losses[-10:])))\n",
    "        plt.plot(losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d260202-f082-4c56-817e-3cb8ffaaa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "ddqn_agent = DDQNAgent(env_id=env_id, eps=0.05, gamma=0.99, lr=5e-4, num_frames=50000, rep_buf_size=10000, batch_size=128, tau=0.02)\n",
    "ddqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af2053-83d3-4c31-b021-6ccd227a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization (Test)\n",
    "\n",
    "env = gym.make(env_id, render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = ddqn_agent.model.greedy_act(np.expand_dims(state, 0))\n",
    "    state, reward, terminated, truncated, _ = env.step(action[0])\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fb54d-1e40-4cd9-8c10-39b844207818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
