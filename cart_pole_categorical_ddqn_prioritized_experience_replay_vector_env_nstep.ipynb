{
 "cells": [
  {
   "cell_type": "raw",
   "id": "95ec85b9-e5a6-4beb-82e6-5dc52500dfd0",
   "metadata": {},
   "source": [
    "Tested with:\n",
    "- python 3.11.5\n",
    "- gymnasium 0.28.1\n",
    "- pytorch 2.1.1\n",
    "- numpy 1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e63e6-8541-4ad3-ab92-c5771261d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38add53d-ce16-4f0c-972c-aa95371f003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = lambda inp: inp.cuda() if USE_CUDA else inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794b2e3-f04e-4c66-ae65-da93cd8faf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT\n",
    "\n",
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38843a0a-2d02-4050-9405-369936ddfec8",
   "metadata": {},
   "source": [
    "Prioritized experience replay\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1511.05952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ee606-5be9-44e1-9043-cb333810ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REPLAY BUFFER\n",
    "\n",
    "class NStepPrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, num_training, n_step, num_env, gamma, prob_alpha=0.6, beta_0=0.4):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.beta_0 = beta_0\n",
    "        self.capacity = (capacity // num_env) * num_env\n",
    "        self.num_training = num_training\n",
    "        self.n_step = n_step\n",
    "        self.num_env = num_env\n",
    "        self.gamma = gamma\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.prio_alp = np.zeros((self.capacity,), dtype=np.float32)\n",
    "        self.max_prio_alp = 1.\n",
    "    \n",
    "    def push(self, batch_state, batch_action, batch_reward, batch_next_state, batch_done):\n",
    "        num_samples = batch_state.shape[0]\n",
    "        assert num_samples == self.num_env, 'The same index of different batches must be from the same environment'\n",
    "        buffer_len = len(self.buffer)\n",
    "        pos = (self.pos + np.arange(num_samples)) % self.capacity\n",
    "        \n",
    "        if buffer_len < self.capacity:\n",
    "            for i in range(num_samples):\n",
    "                self.buffer.append(\n",
    "                    (batch_state[i].reshape(1,-1),\n",
    "                     batch_action[i],\n",
    "                     batch_reward[i],\n",
    "                     batch_next_state[i].reshape(1,-1),\n",
    "                     batch_done[i])\n",
    "                )\n",
    "        else:\n",
    "            if self.prio_alp[pos].max() == self.max_prio_alp:\n",
    "                mask = np.ones_like(self.prio_alp)\n",
    "                mask[pos] = 0\n",
    "                self.max_prio_alp = (self.prio_alp * mask).max()\n",
    "            for i in range(num_samples):\n",
    "                self.buffer[pos[i]] = (\n",
    "                    batch_state[i].reshape(1,-1),\n",
    "                    batch_action[i],\n",
    "                    batch_reward[i],\n",
    "                    batch_next_state[i].reshape(1,-1),\n",
    "                    batch_done[i]\n",
    "                )\n",
    "\n",
    "        self.prio_alp[pos] = self.max_prio_alp\n",
    "        self.pos = (self.pos + num_samples) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, training_idx, model_greedy_func=None):\n",
    "        beta = min(1.0, self.beta_0 + training_idx * (1.0 - self.beta_0) / self.num_training)\n",
    "        buffer_len = len(self.buffer)\n",
    "        \n",
    "        if buffer_len == self.capacity:\n",
    "            prio_alps = self.prio_alp\n",
    "        else:\n",
    "            prio_alps = self.prio_alp[:self.pos]\n",
    "\n",
    "        # P_j = p_j^{\\alpha} / {\\sum_i p_i^{\\alpha}}\n",
    "        probs = prio_alps / prio_alps.sum()\n",
    "\n",
    "        last_indices = np.random.choice(buffer_len, batch_size, p=probs)\n",
    "\n",
    "        # model action\n",
    "        if not(model_greedy_func is None):\n",
    "            model_states = list()\n",
    "            for lidx in last_indices.tolist():\n",
    "                for n in range(0, self.n_step):\n",
    "                    # all wrong indices will be filtered out later\n",
    "                    idx = (lidx - n * self.num_env) % buffer_len\n",
    "                    model_states.append(self.buffer[idx][0])\n",
    "            model_act = model_greedy_func(np.concatenate(model_states)).reshape(-1, self.n_step)\n",
    "        else:\n",
    "            model_act = np.zeros((batch_size, self.n_step))\n",
    "            for i, lidx in enumerate(last_indices.tolist()):\n",
    "                for n in range(0, self.n_step):\n",
    "                    # all wrong indices will be filtered out later\n",
    "                    idx = (lidx - n * self.num_env) % buffer_len\n",
    "                    model_act[i,n] = self.buffer[idx][1]\n",
    "\n",
    "        # get samples\n",
    "        indices = list()\n",
    "        steps = list()\n",
    "        rewards = list()\n",
    "        states = list()\n",
    "        actions = list()\n",
    "        next_states = list()\n",
    "        dones = list()\n",
    "        for i, lidx in enumerate(last_indices.tolist()):\n",
    "            last_ep_idx = lidx\n",
    "            step = 0\n",
    "            reward = 0\n",
    "            next_step_diverge = False\n",
    "            cross_boundary = False\n",
    "            for n in range(0, self.n_step):\n",
    "                if buffer_len == self.capacity:\n",
    "                    idx = (lidx - n * self.num_env) % self.capacity\n",
    "                    idx_diff = (self.pos - idx) % self.capacity\n",
    "                    if (idx_diff <= self.num_env) and (idx_diff > 0) and (n > 0):\n",
    "                        cross_boundary = True\n",
    "                    else:\n",
    "                        cross_boundary = False\n",
    "                else:\n",
    "                    idx = lidx - n * self.num_env\n",
    "                    if idx < 0:\n",
    "                        break\n",
    "                cur_idx_done = self.buffer[idx][4]\n",
    "                if (cross_boundary or cur_idx_done or next_step_diverge) and (n > 0):\n",
    "                    last_ep_idx = idx\n",
    "                    step = 0\n",
    "                    reward = 0\n",
    "                step += 1\n",
    "                reward = self.buffer[idx][2] + self.gamma * reward\n",
    "                indices.append(idx)\n",
    "                steps.append(step)\n",
    "                rewards.append(reward)\n",
    "                states.append(self.buffer[idx][0])\n",
    "                actions.append(self.buffer[idx][1])\n",
    "                next_states.append(self.buffer[last_ep_idx][3])\n",
    "                dones.append(self.buffer[last_ep_idx][4])\n",
    "                target_act = self.buffer[idx][1]\n",
    "                if target_act != model_act[i,n]:\n",
    "                    next_step_diverge = True\n",
    "                else:\n",
    "                    next_step_diverge = False\n",
    "        states = np.concatenate(states)\n",
    "        next_states = np.concatenate(next_states)\n",
    "\n",
    "        # w_j = (N  * P_j)^{-\\beta}\n",
    "        # w_j = w_j / {\\max_i w_i}\n",
    "        weights  = (buffer_len * probs[indices]) ** (-beta)\n",
    "        weights /= (buffer_len * probs.min()) ** (-beta)\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, steps, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        prev_max_prio_alps = self.prio_alp[batch_indices].max()\n",
    "        # p_i^{\\alpha}\n",
    "        prio_alps = batch_priorities ** self.prob_alpha\n",
    "        self.prio_alp[batch_indices] = prio_alps\n",
    "        max_prio_alps = prio_alps.max()\n",
    "        if max_prio_alps > self.max_prio_alp:\n",
    "            self.max_prio_alp = max_prio_alps\n",
    "        elif prev_max_prio_alps == self.max_prio_alp:\n",
    "            self.max_prio_alp = self.prio_alp.max()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6759a-efee-4d21-9857-8c9f20f4604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, v_min, v_max, num_atoms=51):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions * num_atoms)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        z = torch.linspace(v_min, v_max, num_atoms)\n",
    "        self.register_buffer('z', z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.layers(x).view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n",
    "    \n",
    "    def eps_act(self, state, epsilon):\n",
    "        select = np.random.rand(state.shape[0]) > epsilon\n",
    "        action = select * self.greedy_act(state) + np.logical_not(select) * np.random.randint(0, self.num_actions, size=state.shape[0])\n",
    "        return action\n",
    "    \n",
    "    def greedy_act(self, state):\n",
    "        state = device(torch.FloatTensor(state))\n",
    "        with torch.no_grad():\n",
    "            dist = self.forward(state)\n",
    "        q_values = (self.z.view(1,1,-1) * dist).sum(dim=-1)\n",
    "        action = q_values.max(dim=-1)[1].cpu().numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2c1bb-91c4-4703-b02a-a262b181005b",
   "metadata": {},
   "source": [
    "Categorical DDQN\n",
    "\n",
    "Paper:\n",
    "- https://arxiv.org/pdf/1509.06461\n",
    "- https://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf\n",
    "\n",
    "target: $R_{t+1}+{\\gamma}Q\\left(S_{t+1}, \\arg\\max_{a'}{Q\\left(S_{t+1}, a';\\theta_t\\right)};\\theta^-_t\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233bcb-9125-4f2a-b15f-416381cf2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DDQN Agent\n",
    "\n",
    "class CategoricalDDQNAgent:\n",
    "    def __init__(self, env_id, eps, gamma, lr, num_frames, rep_buf_size, batch_size, tgt_upd_delay, n_step, v_min, v_max):\n",
    "        self.envs = gym.vector.make(env_id, num_envs=batch_size)\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.num_frames = num_frames\n",
    "        self.rep_buf_size = rep_buf_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tgt_upd_delay = tgt_upd_delay\n",
    "        self.n_step = n_step\n",
    "        \n",
    "        self.model = device(Net(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n, v_min, v_max))\n",
    "        self.target = device(Net(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n, v_min, v_max))\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.rep_buf = NStepPrioritizedReplayBuffer(rep_buf_size, num_frames, n_step, batch_size, gamma)\n",
    "\n",
    "    def train(self):\n",
    "        losses = []\n",
    "        all_rewards = []\n",
    "        episode_reward = np.zeros(self.batch_size)\n",
    "        \n",
    "        state, _ = self.envs.reset()\n",
    "        for frame_idx in range(0, self.num_frames):\n",
    "            action = self.model.eps_act(state, self.eps)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = self.envs.step(action)\n",
    "            done = np.logical_or(terminated, truncated)\n",
    "            self.rep_buf.push(state, action, reward, next_state, done)\n",
    "\n",
    "            if (frame_idx % self.tgt_upd_delay) == 0:\n",
    "                self.target.load_state_dict(self.model.state_dict())\n",
    "            loss = self.compute_td_loss(frame_idx)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "            all_rewards.extend(episode_reward[done].tolist())\n",
    "            episode_reward[done] = 0\n",
    "                \n",
    "            if (frame_idx + 1) % 200 == 0:\n",
    "                self.plot_training(frame_idx, all_rewards, losses)\n",
    "        \n",
    "        self.envs.close()\n",
    "\n",
    "    def compute_td_loss(self, frame_idx):\n",
    "        state, action, reward, next_state, done, step, indices, weights = \\\n",
    "            self.rep_buf.sample(self.batch_size, frame_idx, self.model.greedy_act)\n",
    "        \n",
    "        state      = device(torch.FloatTensor(state))\n",
    "        next_state = device(torch.FloatTensor(next_state))\n",
    "        action     = device(torch.LongTensor(action))\n",
    "        reward     = device(torch.FloatTensor(reward))\n",
    "        done       = device(torch.FloatTensor(done))\n",
    "        step       = device(torch.FloatTensor(step))\n",
    "        weights    = device(torch.FloatTensor(weights))\n",
    "\n",
    "        model_dist = self.model(state)\n",
    "        model_action = action.view(-1,1,1).expand(-1, -1, self.model.num_atoms) \n",
    "        model_dist = model_dist.gather(1, model_action).squeeze(1)  # [B,n_A]\n",
    "        model_dist.data.clamp_(0.000001, 0.999999)\n",
    "\n",
    "        proj_dist = self.projection_distribution(next_state, reward, done, step)\n",
    "\n",
    "        loss = - (proj_dist * model_dist.log()).sum(dim=-1)\n",
    "        self.rep_buf.update_priorities(indices, np.abs(loss.detach().cpu().numpy()) + 1e-5)\n",
    "        loss = loss * weights\n",
    "        loss = loss.mean()\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def projection_distribution(self, next_states, rewards, dones, steps):\n",
    "        batch_size  = next_states.size(0)\n",
    "        num_atoms = self.model.num_atoms\n",
    "        v_max = self.model.v_max\n",
    "        v_min = self.model.v_min\n",
    "        delta_z = float(v_max - v_min) / (num_atoms - 1)\n",
    "        z = self.model.z\n",
    "    \n",
    "        # get a_{t+1}\n",
    "        with torch.no_grad():\n",
    "            model_next_dist = z * self.model(next_states)  # [B,A,n_A]\n",
    "        model_next_action = model_next_dist.sum(dim=-1).max(1)[1] # [B]\n",
    "        model_next_action = model_next_action.view(-1,1,1).expand(-1, -1, num_atoms)  # [B,1,n_A]\n",
    "    \n",
    "        # get p(x_{t+1},a_{t+1})\n",
    "        with torch.no_grad():\n",
    "            target_next_dist = self.target(next_states) # [B,A,n_A]\n",
    "        target_next_dist = target_next_dist.gather(1, model_next_action).squeeze(1) # [B,n_A]\n",
    "        \n",
    "        Tz = rewards.view(-1,1) + torch.pow(self.gamma, steps).view(-1,1) * (1 - dones).view(-1,1) * z.view(1,-1) # [B,n_A]\n",
    "        Tz = Tz.clamp(min=v_min, max=v_max) # [B,n_A]\n",
    "        b  = (Tz - v_min) / delta_z # [B,n_A]\n",
    "        l  = b.floor().long() # [B,n_A]\n",
    "        u  = b.ceil().long() # [B,n_A]\n",
    "            \n",
    "        offset = device(torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long().view(-1,1).expand(-1, num_atoms)) # [B,n_A]\n",
    "    \n",
    "        proj_dist = torch.zeros_like(target_next_dist) # [B,n_A]   \n",
    "        proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (target_next_dist * (u.float() - b)).view(-1))\n",
    "        proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (target_next_dist * (b - l.float())).view(-1))\n",
    "            \n",
    "        return proj_dist\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(frame_idx, rewards, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('episode: {}, total reward(ma-10): {}'.format(len(rewards), np.mean(rewards[-10:])))\n",
    "        plt.plot(np.array(rewards)[:100 * (len(rewards) // 100)].reshape(-1, 100).mean(axis=1))\n",
    "        plt.subplot(132)\n",
    "        plt.title('frame: {}, loss(ma-10): {:.4f}'.format(frame_idx, np.mean(losses[-10:])))\n",
    "        plt.plot(losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d260202-f082-4c56-817e-3cb8ffaaa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "cat_ddqn_agent = CategoricalDDQNAgent(\n",
    "    env_id=env_id,\n",
    "    eps=0.05,\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    num_frames=50000,\n",
    "    rep_buf_size=10000,\n",
    "    batch_size=128,\n",
    "    tgt_upd_delay=50,\n",
    "    n_step=10,\n",
    "    v_min=0,\n",
    "    v_max=500)\n",
    "cat_ddqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af2053-83d3-4c31-b021-6ccd227a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization (Test)\n",
    "\n",
    "env = gym.make(env_id, render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = cat_ddqn_agent.model.greedy_act(np.expand_dims(state, 0))\n",
    "    state, reward, terminated, truncated, _ = env.step(action[0])\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fb54d-1e40-4cd9-8c10-39b844207818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
