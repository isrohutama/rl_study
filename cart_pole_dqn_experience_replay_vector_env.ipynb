{
 "cells": [
  {
   "cell_type": "raw",
   "id": "95ec85b9-e5a6-4beb-82e6-5dc52500dfd0",
   "metadata": {},
   "source": [
    "Tested with:\n",
    "- python 3.11.5\n",
    "- gymnasium 0.28.1\n",
    "- pytorch 2.1.1\n",
    "- numpy 1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e63e6-8541-4ad3-ab92-c5771261d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38add53d-ce16-4f0c-972c-aa95371f003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = lambda inp: inp.cuda() if USE_CUDA else inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794b2e3-f04e-4c66-ae65-da93cd8faf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT\n",
    "\n",
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ee606-5be9-44e1-9043-cb333810ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REPLAY BUFFER\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6759a-efee-4d21-9857-8c9f20f4604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions)\n",
    "        )\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def eps_act(self, state, epsilon):\n",
    "        select = np.random.rand(state.shape[0]) > epsilon\n",
    "        action = select * self.greedy_act(state) + np.logical_not(select) * np.random.randint(0, self.num_actions, size=state.shape[0])\n",
    "        return action\n",
    "    \n",
    "    def greedy_act(self, state):\n",
    "        state = device(torch.FloatTensor(state))\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "        action = q_values.max(1)[1].cpu().numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2c1bb-91c4-4703-b02a-a262b181005b",
   "metadata": {},
   "source": [
    "DQN\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1312.5602v1.pdf\n",
    "\n",
    "target: $R_{t+1}+\\gamma\\max_{a'}{Q\\left(S_{t+1}, a';\\theta^-_t\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233bcb-9125-4f2a-b15f-416381cf2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN Agent\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_id, eps, gamma, lr, num_frames, rep_buf_size, batch_size, tgt_upd_delay):\n",
    "        self.envs = gym.vector.make(env_id, num_envs=batch_size)\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.num_frames = num_frames\n",
    "        self.rep_buf_size = rep_buf_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tgt_upd_delay = tgt_upd_delay\n",
    "        \n",
    "        self.model = device(Net(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n))\n",
    "        self.target = device(Net(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n))\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.rep_buf = ReplayBuffer(rep_buf_size)\n",
    "\n",
    "    def train(self):\n",
    "        losses = []\n",
    "        all_rewards = []\n",
    "        episode_reward = np.zeros(self.batch_size)\n",
    "        \n",
    "        state, _ = self.envs.reset()\n",
    "        for frame_idx in range(0, self.num_frames):\n",
    "            action = self.model.eps_act(state, self.eps)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = self.envs.step(action)\n",
    "            done = terminated\n",
    "            for i in range(action.shape[0]):\n",
    "                if truncated[i]:\n",
    "                    self.rep_buf.push(state[i,:], action[i], reward[i], info['final_observation'][i], done[i])\n",
    "                else:\n",
    "                    self.rep_buf.push(state[i,:], action[i], reward[i], next_state[i,:], done[i])\n",
    "\n",
    "            if (frame_idx % self.tgt_upd_delay) == 0:\n",
    "                self.target.load_state_dict(self.model.state_dict())\n",
    "            loss = self.compute_td_loss()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "            all_rewards.extend(episode_reward[np.logical_or(done, truncated)].tolist())\n",
    "            episode_reward[np.logical_or(done, truncated)] = 0\n",
    "                \n",
    "            if (frame_idx + 1) % 200 == 0:\n",
    "                self.plot_training(frame_idx, all_rewards, losses)\n",
    "        \n",
    "        self.envs.close()\n",
    "\n",
    "    def compute_td_loss(self):\n",
    "        state, action, reward, next_state, done = self.rep_buf.sample(self.batch_size)\n",
    "        \n",
    "        state      = device(torch.FloatTensor(state))\n",
    "        next_state = device(torch.FloatTensor(next_state))\n",
    "        action     = device(torch.LongTensor(action))\n",
    "        reward     = device(torch.FloatTensor(reward))\n",
    "        done       = device(torch.FloatTensor(done))\n",
    "    \n",
    "        q_values = self.model(state)\n",
    "        q_value  = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target(next_state)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        \n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = self.criterion(q_value, expected_q_value)\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(frame_idx, rewards, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('episode: {}, total reward(ma-10): {}'.format(len(rewards), np.mean(rewards[-10:])))\n",
    "        plt.plot(np.array(rewards)[:100 * (len(rewards) // 100)].reshape(-1, 100).mean(axis=1))\n",
    "        plt.subplot(132)\n",
    "        plt.title('frame: {}, loss(ma-10): {:.4f}'.format(frame_idx, np.mean(losses[-10:])))\n",
    "        plt.plot(losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d260202-f082-4c56-817e-3cb8ffaaa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "dqn_agent = DQNAgent(env_id=env_id, eps=0.05, gamma=0.99, lr=5e-4, num_frames=50000, rep_buf_size=10000, batch_size=128, tgt_upd_delay=50)\n",
    "dqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af2053-83d3-4c31-b021-6ccd227a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization (Test)\n",
    "\n",
    "env = gym.make(env_id, render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = dqn_agent.model.greedy_act(np.expand_dims(state, 0))\n",
    "    state, reward, terminated, truncated, _ = env.step(action[0])\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fb54d-1e40-4cd9-8c10-39b844207818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
