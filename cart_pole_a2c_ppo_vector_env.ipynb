{
 "cells": [
  {
   "cell_type": "raw",
   "id": "95ec85b9-e5a6-4beb-82e6-5dc52500dfd0",
   "metadata": {},
   "source": [
    "Tested with:\n",
    "- python 3.11.5\n",
    "- gymnasium 0.28.1\n",
    "- pytorch 2.1.1\n",
    "- numpy 1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e63e6-8541-4ad3-ab92-c5771261d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38add53d-ce16-4f0c-972c-aa95371f003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = lambda inp: inp.cuda() if USE_CUDA else inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794b2e3-f04e-4c66-ae65-da93cd8faf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT\n",
    "\n",
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6759a-efee-4d21-9857-8c9f20f4604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(ActorNet, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "    \n",
    "    def greedy_act(self, state):\n",
    "        state = device(torch.FloatTensor(state))\n",
    "        with torch.no_grad():\n",
    "            prob = self.forward(state)\n",
    "        action = prob.max(1)[1].cpu().numpy()\n",
    "        return action\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(CriticNet, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.layers(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2c1bb-91c4-4703-b02a-a262b181005b",
   "metadata": {},
   "source": [
    "A2C: Synchronous Advantage Actor Critic\n",
    "\n",
    "$$L_w = \\left[R_t + \\gamma\\hat{v}_w\\left(S_{t+1}\\right) - \\hat{v}_w\\left(S_t\\right)\\right]^2$$\n",
    "$$L_\\theta = -\\left[R_t + \\gamma\\hat{v}_w\\left(S_{t+1}\\right) - \\hat{v}_w\\left(S_t\\right)\\right]\\ln\\pi_\\theta\\left(A_t{\\vert}S_t\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233bcb-9125-4f2a-b15f-416381cf2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A2C Agent\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, env_id, gamma, lr, num_frames, num_steps, batch_size, ppo_epochs, ppo_rel_batch_size, ppo_clip_param):\n",
    "        self.envs = gym.vector.make(env_id, num_envs=batch_size)\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.num_frames = num_frames\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.ppo_batch_size = int(ppo_rel_batch_size * num_steps * batch_size)\n",
    "        self.ppo_clip_param = ppo_clip_param\n",
    "        \n",
    "        self.actor = device(ActorNet(self.envs.single_observation_space.shape[0], self.envs.single_action_space.n))\n",
    "        self.critic = device(CriticNet(self.envs.single_observation_space.shape[0]))\n",
    "        self.optimizer = optim.Adam([{'params': self.actor.parameters()}, {'params': self.critic.parameters()}], lr=lr)\n",
    "\n",
    "    def train(self):\n",
    "        losses = [0.]\n",
    "        all_rewards = []\n",
    "        episode_reward = np.zeros(self.batch_size)\n",
    "        \n",
    "        state, _ = self.envs.reset()\n",
    "        frame_idx = 0\n",
    "        while frame_idx < self.num_frames:\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            not_term_masks = []\n",
    "            trunc_masks = []\n",
    "            next_values = []\n",
    "            states = []\n",
    "            actions = []\n",
    "            \n",
    "            for _ in range(self.num_steps):\n",
    "                state = device(torch.FloatTensor(state))\n",
    "                with torch.no_grad():\n",
    "                    prob = self.actor(state)\n",
    "                    value = self.critic(state)\n",
    "                dist = Categorical(probs=prob)\n",
    "        \n",
    "                action = dist.sample()\n",
    "                next_state, reward, terminated, truncated, info = self.envs.step(action.cpu().numpy())\n",
    "        \n",
    "                log_prob = dist.log_prob(action)\n",
    "                \n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                rewards.append(device(torch.FloatTensor(reward).unsqueeze(1)))\n",
    "                not_term_masks.append(device(torch.FloatTensor(1 - terminated).unsqueeze(1)))\n",
    "                \n",
    "                trunc_masks.append(device(torch.BoolTensor(truncated).unsqueeze(1)))\n",
    "                with torch.no_grad():\n",
    "                    trunc_next_state = np.stack(\n",
    "                        [\n",
    "                            info['final_observation'][i] if truncated[i] else next_state[i,:]\n",
    "                            for i in range(truncated.shape[0])\n",
    "                        ],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    next_values.append(self.critic(device(torch.FloatTensor(trunc_next_state))))\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                \n",
    "                state = next_state\n",
    "                frame_idx += 1\n",
    "\n",
    "                episode_reward += reward\n",
    "                all_rewards.extend(episode_reward[np.logical_or(terminated, truncated)].tolist())\n",
    "                episode_reward[np.logical_or(terminated, truncated)] = 0\n",
    "                \n",
    "                if frame_idx % 200 == 0:\n",
    "                    self.plot_training(frame_idx, all_rewards, losses)\n",
    "\n",
    "            returns = self.compute_returns(rewards, not_term_masks, trunc_masks, next_values)\n",
    "            \n",
    "            log_probs = torch.cat(log_probs)  # [num_steps*batch_size]\n",
    "            returns = torch.cat(returns)  # [num_steps*batch_size]\n",
    "            values = torch.cat(values)  # [num_steps*batch_size]\n",
    "            states = torch.cat(states)  # [num_steps*batch_size]\n",
    "            actions = torch.cat(actions)  # [num_steps*batch_size]\n",
    "        \n",
    "            advantages = returns - values\n",
    "\n",
    "            loss_item = 0.\n",
    "            for _ in range(self.ppo_epochs):\n",
    "                idc = torch.randint(low=0, high=self.num_steps * self.batch_size, size=(self.ppo_batch_size,))\n",
    "                old_state = states[idc, :]\n",
    "                old_action = actions[idc]\n",
    "                old_log_prob = log_probs[idc]\n",
    "                old_return = returns[idc, :]\n",
    "                old_advantage = advantages[idc, :]\n",
    "\n",
    "                new_prob = self.actor(old_state)\n",
    "                new_value = self.critic(old_state)\n",
    "                new_dist = Categorical(probs=new_prob)\n",
    "\n",
    "                new_log_prob = new_dist.log_prob(old_action)\n",
    "                new_entropy = new_dist.entropy().mean()\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                clip1 = ratio * old_advantage\n",
    "                clip2 = torch.clamp(ratio, 1. - self.ppo_clip_param, 1. + self.ppo_clip_param) * old_advantage\n",
    "        \n",
    "                actor_loss  = torch.min(clip1, clip2).mean()\n",
    "                critic_loss = (old_return - new_value).pow(2).mean()\n",
    "        \n",
    "                loss = -actor_loss + 0.5 * critic_loss - 0.001 * new_entropy\n",
    "            \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                loss_item +=loss.item()\n",
    "                \n",
    "            losses.append(loss_item / self.ppo_epochs)\n",
    "        \n",
    "        self.envs.close()\n",
    "\n",
    "    def compute_returns(self, rewards, not_term_masks, trunc_masks, next_values):\n",
    "        R = next_values[-1]\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R[trunc_masks[step]] = (next_values[step])[trunc_masks[step]]  # to handle truncated episode\n",
    "            R = rewards[step] + self.gamma * R * not_term_masks[step]\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(frame_idx, rewards, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('episode: {}, total reward(ma-10): {}'.format(len(rewards), np.mean(rewards[-10:])))\n",
    "        plt.plot(np.array(rewards)[:100 * (len(rewards) // 100)].reshape(-1, 100).mean(axis=1))\n",
    "        plt.subplot(132)\n",
    "        plt.title('frame: {}, loss(ma-10): {:.4f}'.format(frame_idx, np.mean(losses[-10:])))\n",
    "        plt.plot(losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d260202-f082-4c56-817e-3cb8ffaaa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "a2c_agent = A2CAgent(\n",
    "    env_id=env_id,\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    num_frames=50000,\n",
    "    num_steps=10,\n",
    "    batch_size=128,\n",
    "    ppo_epochs=4,\n",
    "    ppo_rel_batch_size=1.,\n",
    "    ppo_clip_param=0.2)\n",
    "a2c_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af2053-83d3-4c31-b021-6ccd227a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization (Test)\n",
    "\n",
    "env = gym.make(env_id, render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = a2c_agent.actor.greedy_act(np.expand_dims(state, 0))\n",
    "    state, reward, terminated, truncated, _ = env.step(action[0])\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fb54d-1e40-4cd9-8c10-39b844207818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
